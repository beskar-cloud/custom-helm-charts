# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Default values for prometheus.
# This is a YAML-formatted file.
# Declare name/value pairs to be passed into your templates.
# name: value

---
images:
  tags:
    apache_proxy: docker.io/library/httpd:2.4
    prometheus: docker.io/prom/prometheus:v2.25.0
    helm_tests: docker.io/openstackhelm/heat:wallaby-ubuntu_focal
    dep_check: quay.io/airshipit/kubernetes-entrypoint:v1.0.0
    image_repo_sync: docker.io/library/docker:17.07.0
  pull_policy: IfNotPresent
  local_registry:
    active: false
    exclude:
      - dep_check
      - image_repo_sync

labels:
  prometheus:
    node_selector_key: openstack-control-plane
    node_selector_value: enabled
  job:
    node_selector_key: openstack-control-plane
    node_selector_value: enabled
  test:
    node_selector_key: openstack-control-plane
    node_selector_value: enabled

pod:
  env:
    prometheus: null
  security_context:
    api:
      pod:
        runAsUser: 65534
      container:
        prometheus_perms:
          runAsUser: 0
          readOnlyRootFilesystem: false
        apache_proxy:
          runAsUser: 0
          readOnlyRootFilesystem: false
        prometheus:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
    test:
      pod:
        runAsUser: 65534
      container:
        prometheus_helm_tests:
          readOnlyRootFilesystem: true
          allowPrivilegeEscalation: false
  affinity:
    anti:
      type:
        default: preferredDuringSchedulingIgnoredDuringExecution
      topologyKey:
        default: kubernetes.io/hostname
      weight:
        default: 10
  mounts:
    prometheus:
      prometheus:
      init_container: null
  replicas:
    prometheus: 1
  lifecycle:
    upgrades:
      statefulsets:
        pod_replacement_strategy: RollingUpdate
    termination_grace_period:
      prometheus:
        timeout: 30
  resources:
    enabled: false
    prometheus:
      limits:
        memory: "1024Mi"
        cpu: "2000m"
      requests:
        memory: "128Mi"
        cpu: "500m"
    jobs:
      image_repo_sync:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      tests:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
  probes:
    prometheus:
      prometheus:
        readiness:
          enabled: true
          params:
            initialDelaySeconds: 30
            timeoutSeconds: 30
        liveness:
          enabled: false
          params:
            initialDelaySeconds: 120
            timeoutSeconds: 30
endpoints:
  cluster_domain_suffix: cluster.local
  local_image_registry:
    name: docker-registry
    namespace: docker-registry
    hosts:
      default: localhost
      internal: docker-registry
      node: localhost
    host_fqdn_override:
      default: null
    port:
      registry:
        node: 5000
  oci_image_registry:
    name: oci-image-registry
    namespace: oci-image-registry
    auth:
      enabled: false
      prometheus:
        username: prometheus
        password: password
    hosts:
      default: localhost
    host_fqdn_override:
      default: null
    port:
      registry:
        default: null
  monitoring:
    name: prometheus
    namespace: null
    auth:
      admin:
        username: admin
        password: changeme
      federate:
        username: federate
        password: changeme
    hosts:
      default: prom-metrics
      public: prometheus
    host_fqdn_override:
      default: null
      # NOTE(srwilkers): this chart supports TLS for fqdn over-ridden public
      # endpoints using the following format:
      # public:
      #   host: null
      #   tls:
      #     crt: null
      #     key: null
    path:
      default: null
    scheme:
      default: 'http'
    port:
      api:
        default: 9090
      http:
        default: 80
  alertmanager:
    name: prometheus-alertmanager
    namespace: null
    hosts:
      default: alerts-engine
      public: prometheus-alertmanager
      discovery: prometheus-alertmanager-discovery
    host_fqdn_override:
      default: null
    path:
      default: null
    scheme:
      default: 'http'
    port:
      api:
        default: 9093
        public: 80
      mesh:
        default: 9094
  ldap:
    hosts:
      default: ldap
    auth:
      admin:
        bind: "cn=admin,dc=cluster,dc=local"
        password: password
    host_fqdn_override:
      default: null
    path:
      default: "/ou=People,dc=cluster,dc=local"
    scheme:
      default: ldap
    port:
      ldap:
        default: 389

dependencies:
  dynamic:
    common:
      local_image_registry:
        jobs:
          - prometheus-image-repo-sync
        services:
          - endpoint: node
            service: local_image_registry
  static:
    image_repo_sync:
      services:
        - endpoint: internal
          service: local_image_registry
    prometheus:
      services: null
    tests:
      services:
        - endpoint: internal
          service: monitoring

monitoring:
  prometheus:
    enabled: true
    prometheus:
      scrape: true

prometheus:
  ## Thanos sidecar container configuration
  ##
  thanos:
    ## @param prometheus.thanos.create Create a Thanos sidecar container
    ##
    create: false
    ## Bitnami Thanos image
    ## ref: https://hub.docker.com/r/bitnami/thanos/tags/
    ## @param prometheus.thanos.image.registry [default: REGISTRY_NAME] Thanos image registry
    ## @param prometheus.thanos.image.repository [default: REPOSITORY_NAME/thanos] Thanos image name
    ## @skip prometheus.thanos.image.tag Thanos image tag
    ## @param prometheus.thanos.image.digest Thanos image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
    ## @param prometheus.thanos.image.pullPolicy Thanos image pull policy
    ## @param prometheus.thanos.image.pullSecrets Specify docker-registry secret names as an array
    ##
    image:
      registry: docker.io
      repository: bitnami/thanos
      tag: 0.32.4-debian-11-r3
      digest: ""
      ## Specify a imagePullPolicy. Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
      ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images
      ##
      pullPolicy: IfNotPresent
      ## Optionally specify an array of imagePullSecrets.
      ## Secrets must be manually created in the namespace.
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
      ## Example:
      ## pullSecrets:
      ##   - myRegistryKeySecretName
      ##
      pullSecrets: []
    ## Thanos Sidecar container's securityContext
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
    ## @param prometheus.thanos.containerSecurityContext.enabled Enable container security context
    ## @param prometheus.thanos.containerSecurityContext.readOnlyRootFilesystem mount / (root) as a readonly filesystem
    ## @param prometheus.thanos.containerSecurityContext.allowPrivilegeEscalation Switch privilegeEscalation possibility on or off
    ## @param prometheus.thanos.containerSecurityContext.runAsNonRoot Force the container to run as a non root user
    ## @param prometheus.thanos.containerSecurityContext.capabilities.drop [array] Linux Kernel capabilities which should be dropped
    ##
    containerSecurityContext:
      enabled: true
      readOnlyRootFilesystem: false
      allowPrivilegeEscalation: false
      runAsNonRoot: true
      capabilities:
        drop:
          - ALL
    ## @param prometheus.thanos.prometheusUrl Override default prometheus url `http://localhost:9090`
    ##
    prometheusUrl: ""
    ## @param prometheus.thanos.extraArgs Additional arguments passed to the thanos sidecar container
    ## extraArgs:
    ## - --log.level=debug
    ## - --tsdb.path=/data/
    ##
    extraArgs: []
    ## @param prometheus.thanos.objectStorageConfig.secretName Support mounting a Secret for the objectStorageConfig of the sideCar container.
    ## @param prometheus.thanos.objectStorageConfig.secretKey Secret key with the configuration file.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/thanos.md
    ## objectStorageConfig:
    ##    secretName: thanos-objstore-config
    ##    secretKey: thanos.yaml
    ##
    objectStorageConfig:
      secretName: ""
      secretKey: thanos.yaml
    ## ref: https://github.com/thanos-io/thanos/blob/main/docs/components/sidecar.md
    ## @param prometheus.thanos.extraVolumeMounts Additional volumeMounts from `prometheus.volumes` for thanos sidecar container
    ## extraVolumeMounts:
    ## - name: my-secret-volume
    ##   mountPath: /etc/thanos/secrets/my-secret
    ##
    extraVolumeMounts: []
    ## Thanos sidecar container resource requests and limits.
    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
    ## We usually recommend not to specify default resources and to leave this as a conscious
    ## choice for the user. This also increases chances charts run on environments with little
    ## resources, such as Minikube. If you do want to specify resources, uncomment the following
    ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    ## @param prometheus.thanos.resources.limits The resources limits for the Thanos sidecar container
    ## @param prometheus.thanos.resources.requests The resources requests for the Thanos sidecar container
    ##
    resources:
      ## Example:
      ## limits:
      ##    cpu: 100m
      ##    memory: 128Mi
      ##
      limits: {}
      ## Examples:
      ## requests:
      ##    cpu: 100m
      ##    memory: 128Mi
      ##
      requests: {}
    ## Configure extra options for liveness probe
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
    ## @param prometheus.thanos.livenessProbe.enabled Turn on and off liveness probe
    ## @param prometheus.thanos.livenessProbe.path Path of the HTTP service for checking the healthy state
    ## @param prometheus.thanos.livenessProbe.initialDelaySeconds Delay before liveness probe is initiated
    ## @param prometheus.thanos.livenessProbe.periodSeconds How often to perform the probe
    ## @param prometheus.thanos.livenessProbe.timeoutSeconds When the probe times out
    ## @param prometheus.thanos.livenessProbe.failureThreshold Minimum consecutive failures for the probe
    ## @param prometheus.thanos.livenessProbe.successThreshold Minimum consecutive successes for the probe
    ##
    livenessProbe:
      enabled: true
      path: /-/healthy
      initialDelaySeconds: 0
      periodSeconds: 5
      timeoutSeconds: 3
      failureThreshold: 120
      successThreshold: 1
    ## Configure extra options for readiness probe
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
    ## @param prometheus.thanos.readinessProbe.enabled Turn on and off readiness probe
    ## @param prometheus.thanos.readinessProbe.path Path of the HTTP service for checking the ready state
    ## @param prometheus.thanos.readinessProbe.initialDelaySeconds Delay before readiness probe is initiated
    ## @param prometheus.thanos.readinessProbe.periodSeconds How often to perform the probe
    ## @param prometheus.thanos.readinessProbe.timeoutSeconds When the probe times out
    ## @param prometheus.thanos.readinessProbe.failureThreshold Minimum consecutive failures for the probe
    ## @param prometheus.thanos.readinessProbe.successThreshold Minimum consecutive successes for the probe
    ##
    readinessProbe:
      enabled: true
      path: /-/ready
      initialDelaySeconds: 0
      periodSeconds: 5
      timeoutSeconds: 3
      failureThreshold: 120
      successThreshold: 1
    ## Thanos Sidecar Service
    ##
    service:
      ## @param prometheus.thanos.service.type Kubernetes service type
      ##
      type: ClusterIP
      ## @param prometheus.thanos.service.ports.grpc Thanos service port
      ##
      ports:
        grpc: 10901
      ## @param prometheus.thanos.service.clusterIP Specific cluster IP when service type is cluster IP. Use `None` to create headless service by default.
      ## Use a "headless" service by default so it returns every pod's IP instead of loadbalancing requests.
      ##
      clusterIP: None
      ## @param prometheus.thanos.service.nodePorts.grpc Specify the nodePort value for the LoadBalancer and NodePort service types.
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
      ## e.g:
      ## nodePort: 30901
      ##
      nodePorts:
        grpc: ""
      ## @param prometheus.thanos.service.loadBalancerIP `loadBalancerIP` if service type is `LoadBalancer`
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
      ##
      loadBalancerIP: ""
      ## @param prometheus.thanos.service.loadBalancerClass Thanos service Load Balancer class if service type is `LoadBalancer` (optional, cloud specific)
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
      ##
      loadBalancerClass: ""
      ## @param prometheus.thanos.service.loadBalancerSourceRanges Address that are allowed when svc is `LoadBalancer`
      ## https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
      ## e.g:
      ## loadBalancerSourceRanges:
      ## - 10.10.10.0/24
      ##
      loadBalancerSourceRanges: []
      ## @param prometheus.thanos.service.labels Additional labels for Thanos service
      ##
      labels: {}
      ## @param prometheus.thanos.service.annotations Additional annotations for Thanos service
      ##
      annotations: {}
      ## @param prometheus.thanos.service.extraPorts Additional ports to expose from the Thanos sidecar container
      ## extraPorts:
      ##   - name: http
      ##     port: 10902
      ##     targetPort: http
      ##     protocol: TCP
      ##
      extraPorts: []
      ## @param prometheus.thanos.service.externalTrafficPolicy Prometheus service external traffic policy
      ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
      ##
      externalTrafficPolicy: Cluster
      ## @param prometheus.thanos.service.sessionAffinity Session Affinity for Kubernetes service, can be "None" or "ClientIP"
      ## If "ClientIP", consecutive client requests will be directed to the same Pod
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies
      ##
      sessionAffinity: None
      ## @param prometheus.thanos.service.sessionAffinityConfig Additional settings for the sessionAffinity
      ## sessionAffinityConfig:
      ##   clientIP:
      ##     timeoutSeconds: 300
      ##
      sessionAffinityConfig: {}
    ## Configure the ingress resource that allows you to access the
    ## Thanos Sidecar installation. Set up the URL
    ## ref: https://kubernetes.io/docs/user-guide/ingress/
    ##
    ingress:
      ## @param prometheus.thanos.ingress.enabled Enable ingress controller resource
      ##
      enabled: false
      ## @param prometheus.thanos.ingress.pathType Ingress path type
      ##
      pathType: ImplementationSpecific
      ## @param prometheus.thanos.ingress.apiVersion Force Ingress API version (automatically detected if not set)
      ##
      apiVersion: ""
      ## @param prometheus.thanos.ingress.hostname Default host for the ingress record
      ##
      hostname: thanos.prometheus.local
      ## @param prometheus.thanos.ingress.path Default path for the ingress record
      ## NOTE: You may need to set this to '/*' in order to use this with ALB ingress controllers
      ##
      path: /
      ## @param prometheus.thanos.ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.
      ## For a full list of possible ingress annotations, please see
      ## ref: https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md
      ## Use this parameter to set the required annotations for cert-manager, see
      ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations
      ##
      ## Examples:
      ## kubernetes.io/ingress.class: nginx
      ## cert-manager.io/cluster-issuer: cluster-issuer-name
      ##
      annotations: {}
      ## @param prometheus.thanos.ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)
      ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster .
      ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/
      ##
      ingressClassName: ""
      ## @param prometheus.thanos.ingress.tls Enable TLS configuration for the host defined at `ingress.hostname` parameter
      ## TLS certificates will be retrieved from a TLS secret with name: `{{- printf "%s-tls" .Values.ingress.hostname }}`
      ## You can:
      ##   - Use the `ingress.secrets` parameter to create this TLS secret
      ##   - Relay on cert-manager to create it by setting `ingress.certManager=true`
      ##   - Relay on Helm to create self-signed certificates by setting `ingress.selfSigned=true`
      ##
      tls: false
      ## @param prometheus.thanos.ingress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm
      ##
      selfSigned: false
      ## @param prometheus.thanos.ingress.extraHosts An array with additional hostname(s) to be covered with the ingress record
      ## e.g:
      ## extraHosts:
      ##   - name: thanos.prometheus.local
      ##     path: /
      ##
      extraHosts: []
      ## @param prometheus.thanos.ingress.extraPaths An array with additional arbitrary paths that may need to be added to the ingress under the main host
      ## e.g:
      ## extraPaths:
      ## - path: /*
      ##   backend:
      ##     serviceName: ssl-redirect
      ##     servicePort: use-annotation
      ##
      extraPaths: []
      ## @param prometheus.thanos.ingress.extraTls TLS configuration for additional hostname(s) to be covered with this ingress record
      ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
      ## e.g:
      ## extraTls:
      ## - hosts:
      ##     - thanos.prometheus.local
      ##   secretName: thanos.prometheus.local-tls
      ##
      extraTls: []
      ## @param prometheus.thanos.ingress.secrets Custom TLS certificates as secrets
      ## NOTE: 'key' and 'certificate' are expected in PEM format
      ## NOTE: 'name' should line up with a 'secretName' set further up
      ## If it is not set and you're using cert-manager, this is unneeded, as it will create a secret for you with valid certificates
      ## If it is not set and you're NOT using cert-manager either, self-signed certificates will be created valid for 365 days
      ## It is also possible to create and manage the certificates outside of this helm chart
      ## Please see README.md for more information
      ## e.g:
      ## secrets:
      ##   - name: thanos.prometheus.local-tls
      ##     key: |-
      ##       -----BEGIN RSA PRIVATE KEY-----
      ##       ...
      ##       -----END RSA PRIVATE KEY-----
      ##     certificate: |-
      ##       -----BEGIN CERTIFICATE-----
      ##       ...
      ##       -----END CERTIFICATE-----
      ##
      secrets: []
      ## @param prometheus.thanos.ingress.extraRules The list of additional rules to be added to this ingress record. Evaluated as a template
      ## Useful when looking for additional customization, such as using different backend
      ##
      extraRules: []

network:
  prometheus:
    ingress:
      public: true
      classes:
        namespace: "nginx"
        cluster: "nginx-cluster"
      annotations:
        nginx.ingress.kubernetes.io/rewrite-target: /
        nginx.ingress.kubernetes.io/affinity: cookie
        nginx.ingress.kubernetes.io/session-cookie-name: kube-ingress-session-prometheus
        nginx.ingress.kubernetes.io/session-cookie-hash: sha1
        nginx.ingress.kubernetes.io/session-cookie-expires: "600"
        nginx.ingress.kubernetes.io/session-cookie-max-age: "600"
    node_port:
      enabled: false
      port: 30900

network_policy:
  prometheus:
    ingress:
      - {}
    egress:
      - {}

secrets:
  oci_image_registry:
    prometheus: prometheus-oci-image-registry-key
  tls:
    monitoring:
      prometheus:
        public: prometheus-tls-public
        internal: prometheus-tls-api

tls_configs:
  # If client certificates are required to connect to metrics endpoints, they
  # can be configured here. They will be mounted in the pod under /tls_configs
  # and can be referenced in scrape configs.
  # The filenames will be the key and subkey concatenanted with a ".", e.g.:
  #   /tls_configs/kubernetes-etcd.ca.pem
  #   /tls_configs/kubernetes-etcd.crt.pem
  #   /tls_configs/kubernetes-etcd.key.pem
  # From the following:
  # kubernetes-etcd:
  #   ca.pem: |
  #     -----BEGIN CERTIFICATE-----
  #     -----END CERTIFICATE-----
  #   crt.pem: |
  #     -----BEGIN CERTIFICATE-----
  #     -----END CERTIFICATE-----
  #   key.pem: |
  #     -----BEGIN RSA PRIVATE KEY-----
  #     -----END RSA PRIVATE KEY-----

storage:
  enabled: true
  pvc:
    name: prometheus-pvc
    access_mode: ["ReadWriteOnce"]
  requests:
    storage: 5Gi
  storage_class: general

manifests:
  certificates: false
  configmap_bin: true
  configmap_etc: true
  ingress: true
  helm_tests: true
  job_image_repo_sync: true
  network_policy: true
  secret_ingress_tls: true
  secret_prometheus: true
  secret_registry: true
  service_ingress: true
  service: true
  statefulset_prometheus: true

conf:
  httpd: |
    ServerRoot "/usr/local/apache2"

    Listen 80

    LoadModule mpm_event_module modules/mod_mpm_event.so
    LoadModule authn_file_module modules/mod_authn_file.so
    LoadModule authn_core_module modules/mod_authn_core.so
    LoadModule authz_host_module modules/mod_authz_host.so
    LoadModule authz_groupfile_module modules/mod_authz_groupfile.so
    LoadModule authz_user_module modules/mod_authz_user.so
    LoadModule authz_core_module modules/mod_authz_core.so
    LoadModule access_compat_module modules/mod_access_compat.so
    LoadModule auth_basic_module modules/mod_auth_basic.so
    LoadModule ldap_module modules/mod_ldap.so
    LoadModule authnz_ldap_module modules/mod_authnz_ldap.so
    LoadModule reqtimeout_module modules/mod_reqtimeout.so
    LoadModule filter_module modules/mod_filter.so
    LoadModule proxy_html_module modules/mod_proxy_html.so
    LoadModule log_config_module modules/mod_log_config.so
    LoadModule env_module modules/mod_env.so
    LoadModule headers_module modules/mod_headers.so
    LoadModule setenvif_module modules/mod_setenvif.so
    LoadModule version_module modules/mod_version.so
    LoadModule proxy_module modules/mod_proxy.so
    LoadModule proxy_connect_module modules/mod_proxy_connect.so
    LoadModule proxy_http_module modules/mod_proxy_http.so
    LoadModule proxy_balancer_module modules/mod_proxy_balancer.so
    LoadModule slotmem_shm_module modules/mod_slotmem_shm.so
    LoadModule slotmem_plain_module modules/mod_slotmem_plain.so
    LoadModule unixd_module modules/mod_unixd.so
    LoadModule status_module modules/mod_status.so
    LoadModule autoindex_module modules/mod_autoindex.so

    <IfModule unixd_module>
    User daemon
    Group daemon
    </IfModule>

    <Directory />
        AllowOverride none
        Require all denied
    </Directory>

    <Files ".ht*">
        Require all denied
    </Files>

    ErrorLog /dev/stderr

    LogLevel warn

    <IfModule log_config_module>
        LogFormat "%a %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-Agent}i\"" combined
        LogFormat "%{X-Forwarded-For}i %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-Agent}i\"" proxy
        LogFormat "%h %l %u %t \"%r\" %>s %b" common

        <IfModule logio_module>
          LogFormat "%a %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-Agent}i\" %I %O" combinedio
        </IfModule>

        SetEnvIf X-Forwarded-For "^.*\..*\..*\..*" forwarded
        CustomLog /dev/stdout common
        CustomLog /dev/stdout combined
        CustomLog /dev/stdout proxy env=forwarded
    </IfModule>

    <Directory "/usr/local/apache2/cgi-bin">
        AllowOverride None
        Options None
        Require all granted
    </Directory>

    <IfModule headers_module>
        RequestHeader unset Proxy early
    </IfModule>

    <IfModule proxy_html_module>
    Include conf/extra/proxy-html.conf
    </IfModule>

    <VirtualHost *:80>
      # Expose metrics to all users, as this is not sensitive information and
      # circumvents the inability of Prometheus to interpolate environment vars
      # in its configuration file
      <Location /metrics>
          ProxyPass http://localhost:{{ tuple "monitoring" "internal" "api" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/metrics
          ProxyPassReverse http://localhost:{{ tuple "monitoring" "internal" "api" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/metrics
          Satisfy Any
          Allow from all
      </Location>
      # Expose the /federate endpoint to all users, as this is also not
      # sensitive information and circumvents the inability of Prometheus to
      # interpolate environment vars in its configuration file
      <Location /federate>
          ProxyPass http://localhost:{{ tuple "monitoring" "internal" "api" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/metrics
          ProxyPassReverse http://localhost:{{ tuple "monitoring" "internal" "api" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/metrics
          Satisfy Any
          Allow from all
      </Location>
      # Restrict general user (LDAP) access to the /graph endpoint, as general trusted
      # users should only be able to query Prometheus for metrics and not have access
      # to information like targets, configuration, flags or build info for Prometheus
      <Location />
          ProxyPass http://localhost:{{ tuple "monitoring" "internal" "api" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/
          ProxyPassReverse http://localhost:{{ tuple "monitoring" "internal" "api" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/
          AuthName "Prometheus"
          AuthType Basic
          AuthBasicProvider file ldap
          AuthUserFile /usr/local/apache2/conf/.htpasswd
          AuthLDAPBindDN {{ .Values.endpoints.ldap.auth.admin.bind }}
          AuthLDAPBindPassword {{ .Values.endpoints.ldap.auth.admin.password }}
          AuthLDAPURL {{ tuple "ldap" "default" "ldap" . | include "helm-toolkit.endpoints.keystone_endpoint_uri_lookup" | quote }}
          Require valid-user
      </Location>
      <Location /graph>
          ProxyPass http://localhost:{{ tuple "monitoring" "internal" "api" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/graph
          ProxyPassReverse http://localhost:{{ tuple "monitoring" "internal" "api" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/graph
          AuthName "Prometheus"
          AuthType Basic
          AuthBasicProvider file ldap
          AuthUserFile /usr/local/apache2/conf/.htpasswd
          AuthLDAPBindDN {{ .Values.endpoints.ldap.auth.admin.bind }}
          AuthLDAPBindPassword {{ .Values.endpoints.ldap.auth.admin.password }}
          AuthLDAPURL {{ tuple "ldap" "default" "ldap" . | include "helm-toolkit.endpoints.keystone_endpoint_uri_lookup" | quote }}
          Require valid-user
      </Location>
      # Restrict access to the /config (dashboard) and /api/v1/status/config (http) endpoints
      # to the admin user
      <Location /config>
          ProxyPass http://localhost:{{ tuple "monitoring" "internal" "api" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/config
          ProxyPassReverse http://localhost:{{ tuple "monitoring" "internal" "api" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/config
          AuthName "Prometheus"
          AuthType Basic
          AuthBasicProvider file
          AuthUserFile /usr/local/apache2/conf/.htpasswd
          Require valid-user
      </Location>
      <Location /api/v1/status/config>
          ProxyPass http://localhost:{{ tuple "monitoring" "internal" "api" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/api/v1/status/config
          ProxyPassReverse http://localhost:{{ tuple "monitoring" "internal" "api" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/api/v1/status/config
          AuthName "Prometheus"
          AuthType Basic
          AuthBasicProvider file
          AuthUserFile /usr/local/apache2/conf/.htpasswd
          Require valid-user
      </Location>
      # Restrict access to the /flags (dashboard) and /api/v1/status/flags (http) endpoints
      # to the admin user
      <Location /flags>
          ProxyPass http://localhost:{{ tuple "monitoring" "internal" "api" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/flags
          ProxyPassReverse http://localhost:{{ tuple "monitoring" "internal" "api" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/flags
          AuthName "Prometheus"
          AuthType Basic
          AuthBasicProvider file
          AuthUserFile /usr/local/apache2/conf/.htpasswd
          Require valid-user
      </Location>
      <Location /api/v1/status/flags>
          ProxyPass http://localhost:{{ tuple "monitoring" "internal" "api" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/api/v1/status/flags
          ProxyPassReverse http://localhost:{{ tuple "monitoring" "internal" "api" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/api/v1/status/flags
          AuthName "Prometheus"
          AuthType Basic
          AuthBasicProvider file
          AuthUserFile /usr/local/apache2/conf/.htpasswd
          Require valid-user
      </Location>
      # Restrict access to the /status (dashboard) endpoint to the admin user
      <Location /status>
          ProxyPass http://localhost:{{ tuple "monitoring" "internal" "api" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/status
          ProxyPassReverse http://localhost:{{ tuple "monitoring" "internal" "api" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/status
          AuthName "Prometheus"
          AuthType Basic
          AuthBasicProvider file
          AuthUserFile /usr/local/apache2/conf/.htpasswd
          Require valid-user
      </Location>
      # Restrict access to the /rules (dashboard) endpoint to the admin user
      <Location /rules>
          ProxyPass http://localhost:{{ tuple "monitoring" "internal" "api" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/rules
          ProxyPassReverse http://localhost:{{ tuple "monitoring" "internal" "api" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/rules
          AuthName "Prometheus"
          AuthType Basic
          AuthBasicProvider file
          AuthUserFile /usr/local/apache2/conf/.htpasswd
          Require valid-user
      </Location>
      # Restrict access to the /targets (dashboard) and /api/v1/targets (http) endpoints
      # to the admin user
      <Location /targets>
          ProxyPass http://localhost:{{ tuple "monitoring" "internal" "api" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/targets
          ProxyPassReverse http://localhost:{{ tuple "monitoring" "internal" "api" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/targets
          AuthName "Prometheus"
          AuthType Basic
          AuthBasicProvider file
          AuthUserFile /usr/local/apache2/conf/.htpasswd
          Require valid-user
      </Location>
      <Location /api/v1/targets>
          ProxyPass http://localhost:{{ tuple "monitoring" "internal" "api" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/api/v1/targets
          ProxyPassReverse http://localhost:{{ tuple "monitoring" "internal" "api" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/api/v1/targets
          AuthName "Prometheus"
          AuthType Basic
          AuthBasicProvider file
          AuthUserFile /usr/local/apache2/conf/.htpasswd
          Require valid-user
      </Location>
      # Restrict access to the /api/v1/admin/tsdb/ endpoints (http) to the admin user.
      # These endpoints are disabled by default, but are included here to ensure only
      # an admin user has access to these endpoints when enabled
      <Location /api/v1/admin/tsdb/>
          ProxyPass http://localhost:{{ tuple "monitoring" "internal" "api" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/api/v1/admin/tsdb/
          ProxyPassReverse http://localhost:{{ tuple "monitoring" "internal" "api" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}/api/v1/admin/tsdb/
          AuthName "Prometheus"
          AuthType Basic
          AuthBasicProvider file
          AuthUserFile /usr/local/apache2/conf/.htpasswd
          Require valid-user
      </Location>
    </VirtualHost>
  prometheus:
    # Consumed by a prometheus helper function to generate the command line flags
    # for configuring the prometheus service
    command_line_flags:
      log.level: info
      query.max_concurrency: 20
      query.timeout: 2m
      storage.tsdb.path: /var/lib/prometheus/data
      storage.tsdb.retention.time: 7d
      # NOTE(srwilkers): These settings default to false, but they are
      # exposed here to allow enabling if desired. Please note the security
      # impacts of enabling these flags. More information regarding the impacts
      # can be found here: https://prometheus.io/docs/operating/security/
      #
      # If set to true, all administrative functionality is exposed via the http
      # /api/*/admin/ path
      web.enable_admin_api: false
      # If set to true, allows for http reloads and shutdown of Prometheus
      web.enable_lifecycle: false
    scrape_configs:
      template: |
        {{- $promHost := tuple "monitoring" "public" . | include "helm-toolkit.endpoints.hostname_fqdn_endpoint_lookup" }}
        {{- if not (empty .Values.conf.prometheus.rules)}}
        rule_files:
        {{- $rulesKeys := keys .Values.conf.prometheus.rules -}}
        {{- range $rule := $rulesKeys }}
          {{ printf "- /etc/config/rules/%s.rules" $rule }}
        {{- end }}
        {{- end }}
        global:
          scrape_interval: 60s
          evaluation_interval: 60s
          external_labels:
            prometheus_host: {{$promHost}}
        scrape_configs:
          - job_name: kubelet
            scheme: https
            # This TLS & bearer token file config is used to connect to the actual scrape
            # endpoints for cluster components. This is separate to discovery auth
            # configuration because discovery & scraping are two separate concerns in
            # Prometheus. The discovery auth config is automatic if Prometheus runs inside
            # the cluster. Otherwise, more config options have to be provided within the
            # <kubernetes_sd_config>.
            tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
            kubernetes_sd_configs:
            - role: node
            scrape_interval: 45s
            relabel_configs:
            - action: labelmap
              regex: __meta_kubernetes_node_label_(.+)
            - target_label: __address__
              replacement: kubernetes.default.svc:443
            - source_labels:
                - __meta_kubernetes_node_name
              regex: (.+)
              target_label: __metrics_path__
              replacement: /api/v1/nodes/${1}/proxy/metrics
            - source_labels:
                - __meta_kubernetes_node_name
              action: replace
              target_label: kubernetes_io_hostname
            # Scrape config for Kubelet cAdvisor.
            #
            # This is required for Kubernetes 1.7.3 and later, where cAdvisor metrics
            # (those whose names begin with 'container_') have been removed from the
            # Kubelet metrics endpoint.  This job scrapes the cAdvisor endpoint to
            # retrieve those metrics.
            #
            # In Kubernetes 1.7.0-1.7.2, these metrics are only exposed on the cAdvisor
            # HTTP endpoint; use "replacement: /api/v1/nodes/${1}:4194/proxy/metrics"
            # in that case (and ensure cAdvisor's HTTP server hasn't been disabled with
            # the --cadvisor-port=0 Kubelet flag).
            #
            # This job is not necessary and should be removed in Kubernetes 1.6 and
            # earlier versions, or it will cause the metrics to be scraped twice.
          - job_name: 'kubernetes-cadvisor'

            # Default to scraping over https. If required, just disable this or change to
            # `http`.
            scheme: https

            # This TLS & bearer token file config is used to connect to the actual scrape
            # endpoints for cluster components. This is separate to discovery auth
            # configuration because discovery & scraping are two separate concerns in
            # Prometheus. The discovery auth config is automatic if Prometheus runs inside
            # the cluster. Otherwise, more config options have to be provided within the
            # <kubernetes_sd_config>.
            tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

            kubernetes_sd_configs:
            - role: node

            relabel_configs:
            - action: labelmap
              regex: __meta_kubernetes_node_label_(.+)
            - target_label: __address__
              replacement: kubernetes.default.svc:443
            - source_labels:
                - __meta_kubernetes_node_name
              regex: (.+)
              target_label: __metrics_path__
              replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
            metric_relabel_configs:
            - source_labels:
                - __name__
              regex: 'container_network_tcp_usage_total'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_tasks_state'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_network_udp_usage_total'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_memory_failures_total'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_cpu_load_average_10s'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_cpu_system_seconds_total'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_cpu_user_seconds_total'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_fs_inodes_free'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_fs_inodes_total'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_fs_io_current'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_fs_io_time_seconds_total'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_fs_io_time_weighted_seconds_total'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_fs_read_seconds_total'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_fs_reads_merged_total'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_fs_reads_merged_total'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_fs_reads_total'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_fs_sector_reads_total'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_fs_sector_writes_total'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_fs_write_seconds_total'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_fs_writes_bytes_total'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_fs_writes_merged_total'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_fs_writes_total'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_last_seen'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_memory_cache'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_memory_failcnt'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_memory_max_usage_bytes'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_memory_rss'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_memory_swap'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_memory_usage_bytes'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_network_receive_errors_total'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_network_receive_packets_dropped_total'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_network_receive_packets_total'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_network_transmit_errors_total'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_network_transmit_packets_dropped_total'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_network_transmit_packets_total'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_spec_cpu_period'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_spec_cpu_shares'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_spec_memory_limit_bytes'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_spec_memory_reservation_limit_bytes'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_spec_memory_swap_limit_bytes'
              action: drop
            - source_labels:
                - __name__
              regex: 'container_start_time_seconds'
              action: drop
            # Scrape config for API servers.
            #
            # Kubernetes exposes API servers as endpoints to the default/kubernetes
            # service so this uses `endpoints` role and uses relabelling to only keep
            # the endpoints associated with the default/kubernetes service using the
            # default named port `https`. This works for single API server deployments as
            # well as HA API server deployments.
          - job_name: 'apiserver'
            kubernetes_sd_configs:
            - role: endpoints
            scrape_interval: 45s
            # Default to scraping over https. If required, just disable this or change to
            # `http`.
            scheme: https
            # This TLS & bearer token file config is used to connect to the actual scrape
            # endpoints for cluster components. This is separate to discovery auth
            # configuration because discovery & scraping are two separate concerns in
            # Prometheus. The discovery auth config is automatic if Prometheus runs inside
            # the cluster. Otherwise, more config options have to be provided within the
            # <kubernetes_sd_config>.
            tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              # If your node certificates are self-signed or use a different CA to the
              # master CA, then disable certificate verification below. Note that
              # certificate verification is an integral part of a secure infrastructure
              # so this should only be disabled in a controlled environment. You can
              # disable certificate verification by uncommenting the line below.
              #
              # insecure_skip_verify: true
            bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
            # Keep only the default/kubernetes service endpoints for the https port. This
            # will add targets for each API server which Kubernetes adds an endpoint to
            # the default/kubernetes service.
            relabel_configs:
            - source_labels:
                - __meta_kubernetes_namespace
                - __meta_kubernetes_service_name
                - __meta_kubernetes_endpoint_port_name
              action: keep
              regex: default;kubernetes;https
            metric_relabel_configs:
            - source_labels:
                - __name__
              regex: 'apiserver_admission_controller_admission_latencies_seconds_bucket'
              action: drop
            - source_labels:
                - __name__
              regex: 'rest_client_request_latency_seconds_bucket'
              action: drop
            - source_labels:
                - __name__
              regex: 'apiserver_response_sizes_bucket'
              action: drop
            - source_labels:
                - __name__
              regex: 'apiserver_admission_step_admission_latencies_seconds_bucket'
              action: drop
            - source_labels:
                - __name__
              regex: 'apiserver_admission_controller_admission_latencies_seconds_count'
              action: drop
            - source_labels:
                - __name__
              regex: 'apiserver_admission_controller_admission_latencies_seconds_sum'
              action: drop
            - source_labels:
                - __name__
              regex: 'apiserver_request_latencies_summary'
              action: drop
          # Scrape config for service endpoints.
          #
          # The relabeling allows the actual service scrape endpoint to be configured
          # via the following annotations:
          #
          # * `prometheus.io/scrape`: Only scrape services that have a value of `true`
          # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
          # to set this to `https` & most likely set the `tls_config` of the scrape config.
          # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
          # * `prometheus.io/port`: If the metrics are exposed on a different port to the
          # service then set this appropriately.
          - job_name: 'openstack-exporter'
            kubernetes_sd_configs:
            - role: endpoints
            scrape_interval: 60s
            relabel_configs:
            - source_labels:
                - __meta_kubernetes_service_name
              action: keep
              regex: "openstack-metrics"
            - source_labels:
                - __meta_kubernetes_service_annotation_prometheus_io_scrape
              action: keep
              regex: true
            - source_labels:
                - __meta_kubernetes_service_annotation_prometheus_io_scheme
              action: replace
              target_label: __scheme__
              regex: (https?)
            - source_labels:
                - __meta_kubernetes_service_annotation_prometheus_io_path
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels:
                - __address__
                - __meta_kubernetes_service_annotation_prometheus_io_port
              action: replace
              target_label: __address__
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: $1:$2
            - action: labelmap
              regex: __meta_kubernetes_service_label_(.+)
            - source_labels:
                - __meta_kubernetes_namespace
              action: replace
              target_label: kubernetes_namespace
            - source_labels:
                - __meta_kubernetes_service_name
              action: replace
              target_label: instance
            - source_labels:
                - __meta_kubernetes_service_name
              action: replace
              target_label: kubernetes_name
            - source_labels:
                - __meta_kubernetes_service_name
              target_label: job
              replacement: ${1}
          - job_name: 'node-exporter'
            kubernetes_sd_configs:
            - role: endpoints
            scrape_interval: 60s
            relabel_configs:
            - source_labels:
                - __meta_kubernetes_service_name
              action: keep
              regex: 'node-exporter'
            - source_labels:
                - __meta_kubernetes_pod_node_name
              action: replace
              target_label: hostname
          - job_name: 'kubernetes-service-endpoints'
            kubernetes_sd_configs:
            - role: endpoints
            scrape_interval: 60s
            relabel_configs:
            - source_labels:
                - __meta_kubernetes_service_name
              action: drop
              regex: '(openstack-metrics|prom-metrics|ceph-mgr|node-exporter)'
            - source_labels:
                - __meta_kubernetes_service_annotation_prometheus_io_scrape
              action: keep
              regex: true
            - source_labels:
                - __meta_kubernetes_service_annotation_prometheus_io_scheme
              action: replace
              target_label: __scheme__
              regex: (https?)
            - source_labels:
                - __meta_kubernetes_service_annotation_prometheus_io_path
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels:
                - __address__
                - __meta_kubernetes_service_annotation_prometheus_io_port
              action: replace
              target_label: __address__
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: $1:$2
            - action: labelmap
              regex: __meta_kubernetes_service_label_(.+)
            - source_labels:
                - __meta_kubernetes_namespace
              action: replace
              target_label: kubernetes_namespace
            - source_labels:
                - __meta_kubernetes_service_name
              action: replace
              target_label: kubernetes_name
            - source_labels:
                - __meta_kubernetes_service_name
              target_label: job
              replacement: ${1}
          # Example scrape config for pods
          #
          # The relabeling allows the actual pod scrape endpoint to be configured via the
          # following annotations:
          #
          # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`
          # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
          # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the
          # pod's declared ports (default is a port-free target if none are declared).
          - job_name: 'kubernetes-pods'
            kubernetes_sd_configs:
            - role: pod
            relabel_configs:
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
              action: keep
              regex: true
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
              action: replace
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: $1:$2
              target_label: __address__
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: kubernetes_namespace
            - source_labels: [__meta_kubernetes_pod_name]
              action: replace
              target_label: kubernetes_pod_name
          - job_name: calico-etcd
            kubernetes_sd_configs:
            - role: service
            scrape_interval: 20s
            relabel_configs:
            - action: labelmap
              regex: __meta_kubernetes_service_label_(.+)
            - action: keep
              source_labels:
                - __meta_kubernetes_service_name
              regex: "calico-etcd"
            - action: keep
              source_labels:
                - __meta_kubernetes_namespace
              regex: kube-system
              target_label: namespace
            - source_labels:
                - __meta_kubernetes_pod_name
              target_label: pod
            - source_labels:
                - __meta_kubernetes_service_name
              target_label: service
            - source_labels:
                - __meta_kubernetes_service_name
              target_label: job
              replacement: ${1}
            - source_labels:
                - __meta_kubernetes_service_label
              target_label: job
              regex: calico-etcd
              replacement: ${1}
            - target_label: endpoint
              replacement: "calico-etcd"
          - job_name: ceph-mgr
            kubernetes_sd_configs:
            - role: service
            scrape_interval: 20s
            relabel_configs:
            - action: labelmap
              regex: __meta_kubernetes_service_label_(.+)
            - action: keep
              source_labels:
                - __meta_kubernetes_service_name
              regex: "ceph-mgr"
            - source_labels:
                - __meta_kubernetes_service_port_name
              action: drop
              regex: 'ceph-mgr'
            - action: keep
              source_labels:
                - __meta_kubernetes_namespace
              regex: ceph
              target_label: namespace
            - source_labels:
                - __meta_kubernetes_pod_name
              target_label: pod
            - source_labels:
                - __meta_kubernetes_service_name
              target_label: service
            - source_labels:
                - __meta_kubernetes_service_name
              target_label: job
              replacement: ${1}
            - source_labels:
                - __meta_kubernetes_service_label
              target_label: job
              regex: ceph-mgr
              replacement: ${1}
            - target_label: endpoint
              replacement: "ceph-mgr"
        alerting:
          alertmanagers:
          - kubernetes_sd_configs:
              - role: pod
            tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
            relabel_configs:
            - source_labels: [__meta_kubernetes_pod_label_application]
              regex: prometheus-alertmanager
              action: keep
            - source_labels: [__meta_kubernetes_pod_container_port_name]
              regex: alerts-api
              action: keep
            - source_labels: [__meta_kubernetes_pod_container_port_name]
              regex: peer-mesh
              action: drop
    rules: []
...
